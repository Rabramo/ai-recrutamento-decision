# Projeto Datathon POSTECH â€“ Fase 5

## DescriÃ§Ã£o do Desafio

Este projeto foi desenvolvido como parte da Fase 5 do Datathon do curso de Data Analytics da POSTECH. O objetivo Ã© propor uma soluÃ§Ã£o baseada em InteligÃªncia Artificial para otimizar o processo de recrutamento e seleÃ§Ã£o da empresa Decision, especializada em serviÃ§os de bodyshop para o setor de tecnologia da informaÃ§Ã£o.

## Objetivo do Projeto

Desenvolver um MVP que utilize dados histÃ³ricos da empresa para prever a compatibilidade entre candidatos e vagas com base nos seguintes critÃ©rios:

- Habilidades tÃ©cnicas (como linguagens de programaÃ§Ã£o)
- AderÃªncia Ã  cultura organizacional da contratante
- Grau de engajamento e motivaÃ§Ã£o do candidato

## Estrutura do Projeto

- `dados/`: arquivos de dados brutos e tratados
- `notebooks/`: notebooks com anÃ¡lises exploratÃ³rias e testes
- `src/`: cÃ³digo-fonte principal para prÃ©-processamento, modelagem e utilitÃ¡rios
- `app/`: aplicaÃ§Ã£o desenvolvida em Streamlit
- `modelos/`: modelos treinados salvos no formato `.pkl`
- `testes/`: testes unitÃ¡rios (caso aplicÃ¡vel)
- `README.md`: documentaÃ§Ã£o do projeto
- `requirements.txt`: lista de dependÃªncias do projeto

## Tecnologias Utilizadas

- Python 3.10
- pandas, numpy, matplotlib, seaborn
- scikit-learn
- streamlit
- joblib
- nltk
- spacy

## InstalaÃ§Ã£o

Clone o repositÃ³rio e instale os pacotes necessÃ¡rios com o seguinte comando:

```bash
pip install -r requirements.txt

### ðŸ“¦ Acesso ao dataset tratado

O arquivo `prospeccao_consolidada.parquet` contÃ©m os dados integrados e normalizados a partir das fontes `candidatos.json`, `vagas.json` e `prospeccoes.json`.

> Tamanho aproximado: 130 MB  
> Acesso pÃºblico via Google Drive

ðŸ”— [Clique aqui para visualizar no Google Drive](https://drive.google.com/file/d/1eQ2TWeGqssR9ImxQw8y83xGl23_o4UN_/view?usp=sharing)

---

### ðŸ“¥ Como carregar no Google Colab

```python
import pandas as pd

# Link direto para leitura do parquet
url = "https://drive.google.com/uc?id=1eQ2TWeGqssR9ImxQw8y83xGl23_o4UN_"

# Carregando com pyarrow
df = pd.read_parquet(url, engine='pyarrow')

# Visualizar amostra
df.head()
